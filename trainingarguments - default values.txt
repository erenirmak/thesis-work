TrainingArguments(self, 
output_dir: str,
overwrite_output_dir: bool = False,
do_train: bool = False,
do_eval: bool = False,
do_predict: bool = False,
eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no',
prediction_loss_only: bool = False,
per_device_train_batch_size: int = 8,
per_device_eval_batch_size: int = 8,
per_gpu_train_batch_size: Optional[int] = None,
per_gpu_eval_batch_size: Optional[int] = None,
gradient_accumulation_steps: int = 1,
eval_accumulation_steps: Optional[int] = None,
eval_delay: Optional[float] = 0,
torch_empty_cache_steps: Optional[int] = None,
learning_rate: float = 5e-05,
weight_decay: float = 0.0,
adam_beta1: float = 0.9,
adam_beta2: float = 0.999,
adam_epsilon: float = 1e-08,
max_grad_norm: float = 1.0,
num_train_epochs: float = 3.0,
max_steps: int = -1,
lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear',
lr_scheduler_kwargs: Union[dict, str, NoneType] = dataclasses._HAS_DEFAULT_FACTORY_CLASS instance,
warmup_ratio: float = 0.0,
warmup_steps: int = 0,
log_level: Optional[str] = 'passive',
log_level_replica: Optional[str] = 'warning',
log_on_each_node: bool = True,
logging_dir: Optional[str] = None,
logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps',
logging_first_step: bool = False,
logging_steps: float = 500,
logging_nan_inf_filter: bool = True,
save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps',
save_steps: float = 500,
save_total_limit: Optional[int] = None,
save_safetensors: Optional[bool] = True,
save_on_each_node: bool = False,
save_only_model: bool = False,
restore_callback_states_from_checkpoint: bool = False,
no_cuda: bool = False,
use_cpu: bool = False,
use_mps_device: bool = False,
seed: int = 42,
data_seed: Optional[int] = None,
jit_mode_eval: bool = False,
use_ipex: bool = False,
bf16: bool = False,
fp16: bool = False,
fp16_opt_level: str = 'O1',
half_precision_backend: str = 'auto',
bf16_full_eval: bool = False,
fp16_full_eval: bool = False,
tf32: Optional[bool] = None,
local_rank: int = -1,
ddp_backend: Optional[str] = None,
tpu_num_cores: Optional[int] = None,
tpu_metrics_debug: bool = False,
debug: Union[str, List[transformers.debug_utils.DebugOption]] = '',
dataloader_drop_last: bool = False,
eval_steps: Optional[float] = None,
dataloader_num_workers: int = 0,
dataloader_prefetch_factor: Optional[int] = None,
past_index: int = -1,
run_name: Optional[str] = None,
disable_tqdm: Optional[bool] = None,
remove_unused_columns: Optional[bool] = True,
label_names: Optional[List[str]] = None,
load_best_model_at_end: Optional[bool] = False,
metric_for_best_model: Optional[str] = None,
greater_is_better: Optional[bool] = None,
ignore_data_skip: bool = False,
fsdp: Union[List[transformers.trainer_utils.FSDPOption], str, NoneType] = '',
fsdp_min_num_params: int = 0,
fsdp_config: Union[dict, str, NoneType] = None,
fsdp_transformer_layer_cls_to_wrap: Optional[str] = None,
accelerator_config: Union[dict, str, NoneType] = None,
deepspeed: Union[dict, str, NoneType] = None,
label_smoothing_factor: float = 0.0,
optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch',
optim_args: Optional[str] = None,
adafactor: bool = False,
group_by_length: bool = False,
length_column_name: Optional[str] = 'length',
report_to: Union[NoneType, str, List[str]] = None,
ddp_find_unused_parameters: Optional[bool] = None,
ddp_bucket_cap_mb: Optional[int] = None,
ddp_broadcast_buffers: Optional[bool] = None,
dataloader_pin_memory: bool = True,
dataloader_persistent_workers: bool = False,
skip_memory_metrics: bool = True,
use_legacy_prediction_loop: bool = False,
push_to_hub: bool = False,
resume_from_checkpoint: Optional[str] = None,
hub_model_id: Optional[str] = None,
hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save',
hub_token: Optional[str] = None,
hub_private_repo: Optional[bool] = None,
hub_always_push: bool = False,
gradient_checkpointing: bool = False,
gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None,
include_inputs_for_metrics: bool = False,
include_for_metrics: List[str] = dataclasses._HAS_DEFAULT_FACTORY_CLASS instance,
eval_do_concat_batches: bool = True,
fp16_backend: str = 'auto',
evaluation_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = None,
push_to_hub_model_id: Optional[str] = None,
push_to_hub_organization: Optional[str] = None,
push_to_hub_token: Optional[str] = None,
mp_parameters: str = '',
auto_find_batch_size: bool = False,
full_determinism: bool = False,
torchdynamo: Optional[str] = None,
ray_scope: Optional[str] = 'last',
ddp_timeout: Optional[int] = 1800,
torch_compile: bool = False,
torch_compile_backend: Optional[str] = None,
torch_compile_mode: Optional[str] = None,
dispatch_batches: Optional[bool] = None,
split_batches: Optional[bool] = None,
include_tokens_per_second: Optional[bool] = False,
include_num_input_tokens_seen: Optional[bool] = False,
neftune_noise_alpha: Optional[float] = None,
optim_target_modules: Union[NoneType, str, List[str]] = None,
batch_eval_metrics: bool = False,
eval_on_start: bool = False,
use_liger_kernel: Optional[bool] = False,
eval_use_gather_object: Optional[bool] = False,
average_tokens_across_devices: Optional[bool] = False)